{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install simpletransformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-09T13:43:07.789389Z","iopub.execute_input":"2023-09-09T13:43:07.789978Z","iopub.status.idle":"2023-09-09T13:43:30.650452Z","shell.execute_reply.started":"2023-09-09T13:43:07.789947Z","shell.execute_reply":"2023-09-09T13:43:30.649363Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting simpletransformers\n  Downloading simpletransformers-0.64.3-py3-none-any.whl (250 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (2.31.0)\nRequirement already satisfied: tqdm>=4.47.0 in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (4.66.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (2023.6.3)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (4.32.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (2.1.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (1.11.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (1.2.2)\nCollecting seqeval (from simpletransformers)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (2.12.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (2.0.2)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (0.13.3)\nRequirement already satisfied: wandb>=0.10.32 in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (0.15.9)\nCollecting streamlit (from simpletransformers)\n  Downloading streamlit-1.26.0-py2.py3-none-any.whl (8.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from simpletransformers) (0.1.99)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (0.16.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (6.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (0.3.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (3.1.31)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->simpletransformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->simpletransformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->simpletransformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->simpletransformers) (2023.7.22)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->simpletransformers) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->simpletransformers) (0.3.7)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->simpletransformers) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->simpletransformers) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets->simpletransformers) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->simpletransformers) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->simpletransformers) (0.18.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->simpletransformers) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->simpletransformers) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->simpletransformers) (2023.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (3.1.0)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.1.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (1.6.2)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (4.2.4)\nRequirement already satisfied: importlib-metadata<7,>=1.4 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (6.7.0)\nRequirement already satisfied: pillow<10,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (9.5.0)\nRequirement already satisfied: pympler<2,>=0.9 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (1.0.1)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (13.4.2)\nRequirement already satisfied: tenacity<9,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (8.2.2)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (4.6.3)\nCollecting tzlocal<5,>=1.1 (from streamlit->simpletransformers)\n  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\nCollecting validators<1,>=0.2 (from streamlit->simpletransformers)\n  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\nCollecting pydeck<1,>=0.8 (from streamlit->simpletransformers)\n  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit->simpletransformers) (6.3.2)\nCollecting watchdog>=2.1.5 (from streamlit->simpletransformers)\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (2.3.7)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard->simpletransformers) (0.40.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.17.3)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.10)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (1.3.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7,>=1.4->streamlit->simpletransformers) (3.15.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->simpletransformers) (3.0.9)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.15.1)\nCollecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit->simpletransformers)\n  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.19.3)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (3.2.2)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=6682b33253dcc62b96ebbb6bebc32201988b29d05aecbd64460a33be55e37d09\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: watchdog, validators, pytz-deprecation-shim, tzlocal, pydeck, seqeval, streamlit, simpletransformers\n  Attempting uninstall: tzlocal\n    Found existing installation: tzlocal 5.0.1\n    Uninstalling tzlocal-5.0.1:\n      Successfully uninstalled tzlocal-5.0.1\nSuccessfully installed pydeck-0.8.0 pytz-deprecation-shim-0.1.0.post0 seqeval-1.2.2 simpletransformers-0.64.3 streamlit-1.26.0 tzlocal-4.3.1 validators-0.22.0 watchdog-3.0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Libraries & Functions","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:43:30.652660Z","iopub.execute_input":"2023-09-09T13:43:30.653291Z","iopub.status.idle":"2023-09-09T13:43:31.013439Z","shell.execute_reply.started":"2023-09-09T13:43:30.653253Z","shell.execute_reply":"2023-09-09T13:43:31.012409Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from simpletransformers.classification import ClassificationModel\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:43:31.016746Z","iopub.execute_input":"2023-09-09T13:43:31.017200Z","iopub.status.idle":"2023-09-09T13:43:49.014544Z","shell.execute_reply.started":"2023-09-09T13:43:31.017167Z","shell.execute_reply":"2023-09-09T13:43:49.013280Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder\n\ndef calculate_scores(y_test, y_pred, average = \"binary\"):\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average = average)\n    recall = recall_score(y_test, y_pred, average = average)\n    f1 = f1_score(y_test, y_pred, average = average)\n    return [accuracy, precision, recall, f1]","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:43:49.015873Z","iopub.execute_input":"2023-09-09T13:43:49.016586Z","iopub.status.idle":"2023-09-09T13:43:49.039807Z","shell.execute_reply.started":"2023-09-09T13:43:49.016543Z","shell.execute_reply":"2023-09-09T13:43:49.038724Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model_names = [\"distilbert\", \"distilbert\", \"bert\", \"bert\"]\ncheckpoint_names = [\"distilbert-base-uncased\", \"distilbert-base-multilingual-cased\", \"dccuchile/bert-base-spanish-wwm-uncased\", \"dbmdz/bert-base-german-uncased\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:48:05.311570Z","iopub.execute_input":"2023-09-09T13:48:05.311940Z","iopub.status.idle":"2023-09-09T13:48:05.317797Z","shell.execute_reply.started":"2023-09-09T13:48:05.311911Z","shell.execute_reply":"2023-09-09T13:48:05.316422Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"focus_names = [\"Growth Form\", \"Life Form\"]\nfocus_codes = [\"1.2.1\", \"2.3.1\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:48:09.901406Z","iopub.execute_input":"2023-09-09T13:48:09.901840Z","iopub.status.idle":"2023-09-09T13:48:09.906456Z","shell.execute_reply.started":"2023-09-09T13:48:09.901807Z","shell.execute_reply":"2023-09-09T13:48:09.905566Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Input Data","metadata":{}},{"cell_type":"code","source":"raw_datasets = dict()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T14:12:31.328176Z","iopub.execute_input":"2023-09-09T14:12:31.328582Z","iopub.status.idle":"2023-09-09T14:12:31.333734Z","shell.execute_reply.started":"2023-09-09T14:12:31.328537Z","shell.execute_reply":"2023-09-09T14:12:31.332472Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def fix_WIKI(name, description):\n    for n in name.split(\" \"):\n        description = str(description).replace(n.lower(), \"\")\n    return description.strip()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T14:12:31.579625Z","iopub.execute_input":"2023-09-09T14:12:31.580245Z","iopub.status.idle":"2023-09-09T14:12:31.586064Z","shell.execute_reply.started":"2023-09-09T14:12:31.580216Z","shell.execute_reply":"2023-09-09T14:12:31.584916Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Spanish Wikipedia - WIKI_GIFT_ESP Dataset","metadata":{}},{"cell_type":"code","source":"working_dir = \"..//input//wiki-gift-esp-final//\" \n\ndf_WIKI_ESP_Cat =  pd.read_excel(working_dir + \"WIKI_GIFT_ESP.xlsx\")\ndf_WIKI_ESP_Cat_Preproc = df_WIKI_ESP_Cat.drop_duplicates(subset = [\"BERT_description\"])\ndf_WIKI_ESP_Cat_Preproc[\"BERT_description\"] = df_WIKI_ESP_Cat_Preproc[[\"name\", \"BERT_description\"]].apply(lambda x: fix_WIKI(x[0], x[1]), axis = 1)\ndf_WIKI_ESP_Cat_Preproc = df_WIKI_ESP_Cat_Preproc[df_WIKI_ESP_Cat_Preproc[\"BERT_description\"].apply(lambda x: len(str(x).split(\" \")))>10]\nraw_datasets[\"WIKI_ESP\"] = df_WIKI_ESP_Cat_Preproc","metadata":{"execution":{"iopub.status.busy":"2023-09-09T14:12:57.768054Z","iopub.execute_input":"2023-09-09T14:12:57.768428Z","iopub.status.idle":"2023-09-09T14:13:02.259237Z","shell.execute_reply.started":"2023-09-09T14:12:57.768398Z","shell.execute_reply":"2023-09-09T14:13:02.258084Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_29/1635896773.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_WIKI_ESP_Cat_Preproc[\"BERT_description\"] = df_WIKI_ESP_Cat_Preproc[[\"name\", \"BERT_description\"]].apply(lambda x: fix_WIKI(x[0], x[1]), axis = 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## German Wikipedia - WIKI_GIFT_DE Dataset","metadata":{}},{"cell_type":"code","source":"working_dir = \"..//input//wiki-gift-de-final//\" \n\ndf_WIKI_DE_Cat =  pd.read_excel(working_dir + \"WIKI_GIFT_DE.xlsx\")\ndf_WIKI_DE_Cat_Preproc = df_WIKI_DE_Cat.drop_duplicates(subset = [\"BERT_description\"])\ndf_WIKI_DE_Cat_Preproc[\"BERT_description\"] = df_WIKI_DE_Cat_Preproc[[\"name\", \"BERT_description\"]].apply(lambda x: fix_WIKI(x[0], x[1]), axis = 1)\ndf_WIKI_DE_Cat_Preproc = df_WIKI_DE_Cat_Preproc[df_WIKI_DE_Cat_Preproc[\"BERT_description\"].apply(lambda x: len(str(x).split(\" \")))>10]\nraw_datasets[\"WIKI_DE\"] = df_WIKI_DE_Cat_Preproc","metadata":{"execution":{"iopub.status.busy":"2023-09-09T14:13:02.261379Z","iopub.execute_input":"2023-09-09T14:13:02.262197Z","iopub.status.idle":"2023-09-09T14:13:04.844869Z","shell.execute_reply.started":"2023-09-09T14:13:02.262162Z","shell.execute_reply":"2023-09-09T14:13:04.843725Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_29/574063204.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_WIKI_DE_Cat_Preproc[\"BERT_description\"] = df_WIKI_DE_Cat_Preproc[[\"name\", \"BERT_description\"]].apply(lambda x: fix_WIKI(x[0], x[1]), axis = 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocess Datasets","metadata":{}},{"cell_type":"code","source":"label_map = {\n    \"Growth Form\": {\"herb\": 0, \"shrub\": 1, \"tree\": 2},\n    \"Life Form\": {\"phanerophyte\": 0, \"chamaephyte\": 1, \"hemicryptophyte\": 2, \"cryptophyte\": 3, \"therophyte\": 4},\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T14:13:04.846498Z","iopub.execute_input":"2023-09-09T14:13:04.846894Z","iopub.status.idle":"2023-09-09T14:13:04.852977Z","shell.execute_reply.started":"2023-09-09T14:13:04.846859Z","shell.execute_reply":"2023-09-09T14:13:04.851399Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"preprocessed_dataset_dict = {}\nfor focus_name, focus_code in zip(focus_names, focus_codes):\n    for dataset_name in list(raw_datasets.keys()):\n        labelencoder = LabelEncoder()\n\n        dataset_masked = raw_datasets[dataset_name][raw_datasets[dataset_name][focus_code].notna()]\n        dataset_masked = dataset_masked[dataset_masked[focus_code].apply(lambda x: x in label_map[focus_name].keys())]\n        dataset_masked[focus_code + \"_encoded\"] = labelencoder.fit_transform(dataset_masked[focus_code])\n\n        indices_train, indices_test \\\n            = train_test_split(dataset_masked.index.values, test_size=0.25, random_state=42)\n            \n        df_train = dataset_masked.loc[indices_train, [\"BERT_description\", focus_code + \"_encoded\"]]\n        df_train.columns = [\"text\", \"labels\"]\n        df_test = dataset_masked.loc[indices_test, [\"BERT_description\", focus_code + \"_encoded\"]]\n        df_test.columns = [\"text\", \"labels\"]\n        \n        preprocessed_dataset_dict[dataset_name, focus_name, \"train\"] = df_train\n        preprocessed_dataset_dict[dataset_name, focus_name, \"validation\"] = df_test","metadata":{"execution":{"iopub.status.busy":"2023-09-09T14:13:04.855455Z","iopub.execute_input":"2023-09-09T14:13:04.856157Z","iopub.status.idle":"2023-09-09T14:13:04.908402Z","shell.execute_reply.started":"2023-09-09T14:13:04.856124Z","shell.execute_reply":"2023-09-09T14:13:04.907264Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Model Training & Evaluation","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():  # Tell PyTorch to use the GPU. \n    device = torch.device(\"cuda\") \n    print('There are %d GPU(s) available.' % torch.cuda.device_count()) \n    print('We will use the GPU:', torch.cuda.get_device_name(0)) # If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n    \nimport gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T14:13:04.909842Z","iopub.execute_input":"2023-09-09T14:13:04.910398Z","iopub.status.idle":"2023-09-09T14:13:05.249428Z","shell.execute_reply.started":"2023-09-09T14:13:04.910361Z","shell.execute_reply":"2023-09-09T14:13:05.248448Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"There are 2 GPU(s) available.\nWe will use the GPU: Tesla T4\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"4796"},"metadata":{}}]},{"cell_type":"code","source":"results_list = []\n\nfor model_name, model_checkpoint in zip(model_names[:], checkpoint_names[:]):\n    for focus_name, focus_code in zip(focus_names, focus_codes):\n        for dataset_name in list(raw_datasets.keys())[:]:\n\n            print(model_name, focus_name, dataset_name)\n\n            model = ClassificationModel(\n                model_name,\n                model_checkpoint,\n                num_labels = preprocessed_dataset_dict[dataset_name, focus_name, \"train\"][\"labels\"].nunique(),\n                args = {\"num_train_epochs\": 3, \"train_batch_size\":8, \"eval_batch_size\":8, \"reprocess_input_data\": True, \"overwrite_output_dir\": True, \"save_model_every_epoch\": False, \"save_eval_checkpoints\": False, \"max_seq_length\": 512}, #\"weight_decay\": 0.01, \"learning_rate\": 2e-5, \n            )\n            # Train the model\n            model.train_model(preprocessed_dataset_dict[dataset_name, focus_name, \"train\"])\n\n            # Evaluate the model\n            result, model_outputs, wrong_predictions = model.eval_model(preprocessed_dataset_dict[dataset_name, focus_name, \"validation\"])\n            preprocessed_dataset_dict[dataset_name, focus_name, \"validation\"][\"prediction\"] = np.argmax(model_outputs, axis=1)\n            results = calculate_scores(preprocessed_dataset_dict[dataset_name, focus_name, \"validation\"][\"labels\"], preprocessed_dataset_dict[dataset_name, focus_name, \"validation\"][\"prediction\"], average = \"macro\")\n            results_list.append([dataset_name, focus_name] + results + [model_checkpoint])\n\n            torch.cuda.empty_cache()\n            gc.collect()\n            print(results)\n\ndf_results = pd.DataFrame(results_list, columns=[\"Dataset\", \"Trait\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Model\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-09T14:13:13.400321Z","iopub.execute_input":"2023-09-09T14:13:13.400743Z","iopub.status.idle":"2023-09-09T15:07:05.359455Z","shell.execute_reply.started":"2023-09-09T14:13:13.400709Z","shell.execute_reply":"2023-09-09T15:07:05.358238Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"distilbert Growth Form WIKI_ESP\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2930 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68622da4dc54415082635c4afdffe679"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9249571f90c444fbb1ef181a4162e7d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e59efb5271b84a30bfe8ea4d237e475c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40beed39fe5a4b448df6ad53a4b24392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ee334643af45c9b154c64eb88df8b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/977 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e521b58fe64184930f39f7aaab3b2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/123 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e4f0fdde834453bb4f2c1f2cb96536"}},"metadata":{}},{"name":"stdout","text":"[0.8505629477993859, 0.8204877550860167, 0.807822229679716, 0.8136386669114092]\ndistilbert Growth Form WIKI_DE\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1736 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"771103e9464847cd898e1bb32ec2f1b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f28bbb62b3b4b82a4d098856bdcbfce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c3ffe2666364edc8363e0724e4ee6a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b57462d27b744b0910ddfe0da1e69f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f83a04e6fa06430583ddf8e155d8a970"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/579 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5afca0a17ab54f39b99288527d4655ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/73 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd528f83cccd4804bf5a509d7a539fa7"}},"metadata":{}},{"name":"stdout","text":"[0.8635578583765112, 0.820741474531668, 0.8277310889771612, 0.8234292033788139]\ndistilbert Life Form WIKI_ESP\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1758 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84195559ff084c2f8bce078afe9c91cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40e3c4dd5e4740e0a0055b66e6cb77d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"548bc2473b354e12b8096f84fcea7046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c13be2d23ebd43e3b88f3ea60e4cf460"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"688d32f521f2426bb237554babacaedf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/586 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a81f1e46c864214861182012fd22fc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/74 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fea194d45ac14287989897ab609aa524"}},"metadata":{}},{"name":"stdout","text":"[0.6825938566552902, 0.6082153454560805, 0.5868440040827249, 0.5935761329004402]\ndistilbert Life Form WIKI_DE\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1275 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b225284a3886495282df16cdcd0c15d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4577f1e8c64a7ebe7e2d4d132f8dca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfc37e89930b443eb49f8f250384fb30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"284880bbd57b47eba9390ff7840181aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd2e8b274952486190713a1f9b90d349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/426 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87de2f9a9e3448b0b37c9d905450c889"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6285a9a42774293b8b2515c144b8421"}},"metadata":{}},{"name":"stdout","text":"[0.6737089201877934, 0.7232015175835401, 0.5613722227470699, 0.5228100287287674]\ndistilbert Growth Form WIKI_ESP\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2930 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a636a89ea5bb487887a8638baa216464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eeec285e35a428aa56a8115bc030aee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"386657574170411382dccc9c1f2f84c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2021c87a188432e85fa753e85638a19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a18783d20f548129200610c97e90fcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/977 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a947f20667440588b8a8ed80fc3889e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/123 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a33a2b2d8f540ab99b5bfb38a66699e"}},"metadata":{}},{"name":"stdout","text":"[0.8505629477993859, 0.8199409086505861, 0.8072098395727555, 0.8130010069111705]\ndistilbert Growth Form WIKI_DE\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1736 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dbf4b6c099c44b0bc66f1699d191a2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"563ad379e7d04a38bf11b9ff56fe6fbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f35d81c97f4f32ad992f4b3036746c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0246e2855aa2446687b213eaaa3cc52a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3181f9d2c884c19908ad4e9f874c638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/579 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e26cdb4fbd4c44f5bb2e98dfdd0c3b75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/73 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a486b5d606c4b7aacd26cede61faf70"}},"metadata":{}},{"name":"stdout","text":"[0.8946459412780656, 0.8632308777887624, 0.843061539187851, 0.8524582909986153]\ndistilbert Life Form WIKI_ESP\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1758 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43e03ab15a8e41f8906b4a4e2c8c00ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a40e4bd22be0444c865a66f892502f25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"699fac102b70489c84917b540a560910"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"251913da02594eb1b8bfa521af832219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8aa692ac6c6447dafeeca98f2307fd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/586 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"811a757867d34cb780b1bfcde57e9982"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/74 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba3ed15a7d8b452cac5b1663de886c45"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"[0.537542662116041, 0.3714794858870688, 0.42208145937443475, 0.3759266865237886]\ndistilbert Life Form WIKI_DE\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1275 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85076bf54ea548cc875a6701375af5b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d3188db11a43db9958c03c368fa2e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bd0c09cb4724659b63a77f27613715e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"636aa2d291f743249f61b87987575f84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d76411d060b4ca0a7a92bf036adbe90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/426 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f33aa08c2264b77a72dc972c3282c9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"259e9e46ff314257bfe9b3df5de09951"}},"metadata":{}},{"name":"stdout","text":"[0.6056338028169014, 0.5049861933305395, 0.47397340814415106, 0.4731867517743032]\nbert Growth Form WIKI_ESP\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f278a815972a4d678b06f66424f33947"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae6fdb0a32aa4acb95c72ea1fee5e85c"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/310 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a26fc39e6174d3e9a36c4ce855e54db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a655e9e598f845edb296b9b9142ac3b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/486k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d0e9019ee6f4f4f84bd81eda8f87d18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4beb1c56182499fb57253310bc04dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2930 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9251f6723d4422a90f7f11c2f8eb896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8c19237bf6941429bab29b4d0b606e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e049d5a2896b43bbb5362e40cbe4ebae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63a9e50b2230417b94ff98b854de2080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9de211da1e654ce4b3c9e1b50144afd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/977 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb8e259673f4a98bbba2660ead364a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/123 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c195e2b5dd424e6dab4cad84f15dc824"}},"metadata":{}},{"name":"stdout","text":"[0.8536335721596725, 0.8261857523040974, 0.8103626844621358, 0.8175239518712513]\nbert Growth Form WIKI_DE\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1736 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26a20403807145fdacae6eeee8e2199a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510fd05d468b4f1a9049eb35f4c64a0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f7cc82e534e401ea2cfb6a97bff7d47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ba861906654ff3ba9057f22f41e40d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96070658230d4b5fb7eaae7c060b5d94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/579 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1015ececdb6f4f1a8cf5d2fc179fff20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/73 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"645491b8fd9e485395d7e66b1f2ad2b2"}},"metadata":{}},{"name":"stdout","text":"[0.8739205526770294, 0.8397964083357342, 0.8110187765283515, 0.8239403262223707]\nbert Life Form WIKI_ESP\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1758 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3eb976351ac41b9be336334761e4d5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3522b5dbc4456f82d4984152946bfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d7869bf2ee6420ca88b225894ce73cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c210296030642e9bf91c68a367d4741"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8221a17d7644470a13bfd283fa091bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/586 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d39b76a7bb2f499cacfa0753893a1c50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/74 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69215eb6c8ea42fbacf3e5c474a51600"}},"metadata":{}},{"name":"stdout","text":"[0.7491467576791809, 0.6783950990942517, 0.674318129044, 0.6751570463060675]\nbert Life Form WIKI_DE\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1275 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59539ddd458a485888864171b58bc5af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa71b96dc0d48fe861acec66ce5d9e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4065193f68634f3e9a1e0b409bb9e657"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d965e57a7c941b4910a8912b0805607"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45cf8ee56d5648bd83b097e7bc45c2d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/426 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90007ce3f0654a75b87eddd76a1eacba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96051a3dd21d4b28a7160d8e05455e93"}},"metadata":{}},{"name":"stdout","text":"[0.6901408450704225, 0.62196541421707, 0.6219347298798238, 0.6143789386904207]\nbert Growth Form WIKI_ESP\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24887699c66b46bfb6d2e5ca8115a8c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eac3263a997415aab8e370cb35f25ff"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de29830d6ffa4c808dcda3e7b80e06ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/247k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05eef14277b8486e9301a26a8505706d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2930 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8153844c55e141b59d0669725ef3c64f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c7c72ae9704eca87f983caae801c78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"205a2bd6363e49969131e92975d05f02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e4f54a074ba4495a01007ffc4b3c7bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2654edbe75d48a6b519b79a3bf66e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/977 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09fdc465566240c9853e628056edfe89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/123 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11f45ed6b71e4716a2f5b5b4aae8bf4c"}},"metadata":{}},{"name":"stdout","text":"[0.8331627430910952, 0.8083719959316552, 0.7757317263024515, 0.7879857551027426]\nbert Growth Form WIKI_DE\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1736 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b1886914afb4b3a96265fcc3947cdd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a8e19660a414404a043f0352d2ead5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a49c6d1fb7c4860a5c8767f1029c656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"623e2809294944fbadeb4facbea602a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/217 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fe4f1d57b3c4d46b4db1b1966ba82c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/579 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef87e0e82744daaa1089469f72cb055"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/73 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d83597dd35b8475cb2d1f069bf33612d"}},"metadata":{}},{"name":"stdout","text":"[0.9032815198618307, 0.8739190055326377, 0.8671327586850501, 0.8704495885578236]\nbert Life Form WIKI_ESP\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1758 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c89f3dfac2e0445e9cc9310c99c745e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cf858abe7cd48299e235106fc23641b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac5af52bf96d4cd582c23613ce9dc439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e186a22a8d8446c88c782e3be5127c19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/220 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f59ea20698e4893bedfc9d02979d45c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/586 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f5eed202a81402eb94780de6a194839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/74 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"058cfd2a08bd4f1ebda5bfcd457cc724"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"[0.5, 0.37410169578738495, 0.3583363951867664, 0.30174272063335056]\nbert Life Form WIKI_DE\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1275 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"472f4d7c843f4eb0bf3336f726103e72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6421face864d75a8545f0b6d7a5853"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6eaaef8f3e145a1872f1a14834029ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4669ac88341045b2a54419a8b1a428fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/160 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd0f420a58ef49fe95a6a384e142c092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/426 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b6a0ca8169475e817b481726bce4b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f03caaa24e34e53b964a1c078a12d6e"}},"metadata":{}},{"name":"stdout","text":"[0.8685446009389671, 0.8634695780634425, 0.8052200621706891, 0.8221444317849576]\n","output_type":"stream"}]},{"cell_type":"code","source":"df_results","metadata":{"execution":{"iopub.status.busy":"2023-09-09T15:07:05.361604Z","iopub.execute_input":"2023-09-09T15:07:05.362057Z","iopub.status.idle":"2023-09-09T15:07:05.388431Z","shell.execute_reply.started":"2023-09-09T15:07:05.362016Z","shell.execute_reply":"2023-09-09T15:07:05.387338Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"     Dataset        Trait  Accuracy  Precision    Recall  F1-Score  \\\n0   WIKI_ESP  Growth Form  0.850563   0.820488  0.807822  0.813639   \n1    WIKI_DE  Growth Form  0.863558   0.820741  0.827731  0.823429   \n2   WIKI_ESP    Life Form  0.682594   0.608215  0.586844  0.593576   \n3    WIKI_DE    Life Form  0.673709   0.723202  0.561372  0.522810   \n4   WIKI_ESP  Growth Form  0.850563   0.819941  0.807210  0.813001   \n5    WIKI_DE  Growth Form  0.894646   0.863231  0.843062  0.852458   \n6   WIKI_ESP    Life Form  0.537543   0.371479  0.422081  0.375927   \n7    WIKI_DE    Life Form  0.605634   0.504986  0.473973  0.473187   \n8   WIKI_ESP  Growth Form  0.853634   0.826186  0.810363  0.817524   \n9    WIKI_DE  Growth Form  0.873921   0.839796  0.811019  0.823940   \n10  WIKI_ESP    Life Form  0.749147   0.678395  0.674318  0.675157   \n11   WIKI_DE    Life Form  0.690141   0.621965  0.621935  0.614379   \n12  WIKI_ESP  Growth Form  0.833163   0.808372  0.775732  0.787986   \n13   WIKI_DE  Growth Form  0.903282   0.873919  0.867133  0.870450   \n14  WIKI_ESP    Life Form  0.500000   0.374102  0.358336  0.301743   \n15   WIKI_DE    Life Form  0.868545   0.863470  0.805220  0.822144   \n\n                                      Model  \n0                   distilbert-base-uncased  \n1                   distilbert-base-uncased  \n2                   distilbert-base-uncased  \n3                   distilbert-base-uncased  \n4        distilbert-base-multilingual-cased  \n5        distilbert-base-multilingual-cased  \n6        distilbert-base-multilingual-cased  \n7        distilbert-base-multilingual-cased  \n8   dccuchile/bert-base-spanish-wwm-uncased  \n9   dccuchile/bert-base-spanish-wwm-uncased  \n10  dccuchile/bert-base-spanish-wwm-uncased  \n11  dccuchile/bert-base-spanish-wwm-uncased  \n12           dbmdz/bert-base-german-uncased  \n13           dbmdz/bert-base-german-uncased  \n14           dbmdz/bert-base-german-uncased  \n15           dbmdz/bert-base-german-uncased  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>Trait</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1-Score</th>\n      <th>Model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WIKI_ESP</td>\n      <td>Growth Form</td>\n      <td>0.850563</td>\n      <td>0.820488</td>\n      <td>0.807822</td>\n      <td>0.813639</td>\n      <td>distilbert-base-uncased</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WIKI_DE</td>\n      <td>Growth Form</td>\n      <td>0.863558</td>\n      <td>0.820741</td>\n      <td>0.827731</td>\n      <td>0.823429</td>\n      <td>distilbert-base-uncased</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>WIKI_ESP</td>\n      <td>Life Form</td>\n      <td>0.682594</td>\n      <td>0.608215</td>\n      <td>0.586844</td>\n      <td>0.593576</td>\n      <td>distilbert-base-uncased</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>WIKI_DE</td>\n      <td>Life Form</td>\n      <td>0.673709</td>\n      <td>0.723202</td>\n      <td>0.561372</td>\n      <td>0.522810</td>\n      <td>distilbert-base-uncased</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>WIKI_ESP</td>\n      <td>Growth Form</td>\n      <td>0.850563</td>\n      <td>0.819941</td>\n      <td>0.807210</td>\n      <td>0.813001</td>\n      <td>distilbert-base-multilingual-cased</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>WIKI_DE</td>\n      <td>Growth Form</td>\n      <td>0.894646</td>\n      <td>0.863231</td>\n      <td>0.843062</td>\n      <td>0.852458</td>\n      <td>distilbert-base-multilingual-cased</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>WIKI_ESP</td>\n      <td>Life Form</td>\n      <td>0.537543</td>\n      <td>0.371479</td>\n      <td>0.422081</td>\n      <td>0.375927</td>\n      <td>distilbert-base-multilingual-cased</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>WIKI_DE</td>\n      <td>Life Form</td>\n      <td>0.605634</td>\n      <td>0.504986</td>\n      <td>0.473973</td>\n      <td>0.473187</td>\n      <td>distilbert-base-multilingual-cased</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>WIKI_ESP</td>\n      <td>Growth Form</td>\n      <td>0.853634</td>\n      <td>0.826186</td>\n      <td>0.810363</td>\n      <td>0.817524</td>\n      <td>dccuchile/bert-base-spanish-wwm-uncased</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>WIKI_DE</td>\n      <td>Growth Form</td>\n      <td>0.873921</td>\n      <td>0.839796</td>\n      <td>0.811019</td>\n      <td>0.823940</td>\n      <td>dccuchile/bert-base-spanish-wwm-uncased</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>WIKI_ESP</td>\n      <td>Life Form</td>\n      <td>0.749147</td>\n      <td>0.678395</td>\n      <td>0.674318</td>\n      <td>0.675157</td>\n      <td>dccuchile/bert-base-spanish-wwm-uncased</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>WIKI_DE</td>\n      <td>Life Form</td>\n      <td>0.690141</td>\n      <td>0.621965</td>\n      <td>0.621935</td>\n      <td>0.614379</td>\n      <td>dccuchile/bert-base-spanish-wwm-uncased</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>WIKI_ESP</td>\n      <td>Growth Form</td>\n      <td>0.833163</td>\n      <td>0.808372</td>\n      <td>0.775732</td>\n      <td>0.787986</td>\n      <td>dbmdz/bert-base-german-uncased</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>WIKI_DE</td>\n      <td>Growth Form</td>\n      <td>0.903282</td>\n      <td>0.873919</td>\n      <td>0.867133</td>\n      <td>0.870450</td>\n      <td>dbmdz/bert-base-german-uncased</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>WIKI_ESP</td>\n      <td>Life Form</td>\n      <td>0.500000</td>\n      <td>0.374102</td>\n      <td>0.358336</td>\n      <td>0.301743</td>\n      <td>dbmdz/bert-base-german-uncased</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>WIKI_DE</td>\n      <td>Life Form</td>\n      <td>0.868545</td>\n      <td>0.863470</td>\n      <td>0.805220</td>\n      <td>0.822144</td>\n      <td>dbmdz/bert-base-german-uncased</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_results.to_excel(\"outputs//NonEnglishCategoricalTraitClassification_Encoder_Results.xlsx\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T15:10:28.392417Z","iopub.execute_input":"2023-09-09T15:10:28.392837Z","iopub.status.idle":"2023-09-09T15:10:28.420467Z","shell.execute_reply.started":"2023-09-09T15:10:28.392805Z","shell.execute_reply":"2023-09-09T15:10:28.419557Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2023-09-09T15:08:37.989072Z","iopub.execute_input":"2023-09-09T15:08:37.989466Z","iopub.status.idle":"2023-09-09T15:08:39.209740Z","shell.execute_reply.started":"2023-09-09T15:08:37.989437Z","shell.execute_reply":"2023-09-09T15:08:39.208509Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"NonEnglishCategoricalTraitClassification_Encoder_Results.xlsx  outputs\ncache_dir\t\t\t\t\t\t       runs\n","output_type":"stream"}]}]}